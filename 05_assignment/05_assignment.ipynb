{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Важно!** \n",
    "\n",
    "Домашнее задание состоит из нескольких задач, которые вам нужно решить.\n",
    "*   Баллы выставляются по принципу выполнено/невыполнено.\n",
    "*   За каждую выполненую задачу вы получаете баллы (количество баллов за задание указано в скобках).\n",
    "\n",
    "**Инструкция выполнения:** Выполните задания в этом же ноутбуке (места под решения **КАЖДОЙ** задачи обозначены как **#НАЧАЛО ВАШЕГО РЕШЕНИЯ** и **#КОНЕЦ ВАШЕГО РЕШЕНИЯ**)\n",
    "\n",
    "**Как отправить задание на проверку:** Вам необходимо сохранить ваше решение в данном блокноте и отправить итоговый **файл .IPYNB** в личном сообщении Telegram.\n",
    "\n",
    "# **Прежде чем проверять задания:**\n",
    "\n",
    "1. Перезапустите **ядро (restart the kernel)**: в меню, выбрать **Ядро (Kernel)**\n",
    "→ **Перезапустить (Restart)**\n",
    "2. Затем **Выполнить** **все ячейки (run all cells)**: в меню, выбрать **Ячейка (Cell)**\n",
    "→ **Запустить все (Run All)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "В данном задании основное внимание уделяется изучению и применению современных методов **Parameter Efficient Fine-Tuning (PEFT)**. Эти подходы позволяют эффективно дообучать большие языковые модели, используя лишь небольшую часть параметров, что критически важно при работе с ограниченными вычислительными ресурсами.\n",
    "\n",
    "### Обязательные PEFT методы для изучения:\n",
    "\n",
    "1. **LoRA (Low-Rank Adaptation)**\n",
    "   - Разложение весовых матриц на произведение матриц низкого ранга\n",
    "   - Ключевые гиперпараметры: `r` (rank), `alpha`, `dropout`, `target_modules`\n",
    "\n",
    "2. **QLoRA (Quantized LoRA)** \n",
    "   - Комбинация 4-bit квантизации (NF4) с LoRA\n",
    "   - Значительно снижает потребление GPU памяти\n",
    "\n",
    "3. **AdaLoRA (Adaptive LoRA)**\n",
    "   - Динамическое изменение ранга во время обучения\n",
    "   - Автоматическая оптимизация распределения параметров\n",
    "\n",
    "### Дополнительные методы (по выбору):\n",
    "- **IA³ (Infused Adapter by Inhibiting and Amplifying Inner Activations)**\n",
    "- **Prefix Tuning / P-Tuning v2**\n",
    "- **Prompt Tuning**\n",
    "\n",
    "### Методология сравнения PEFT подходов:\n",
    "\n",
    "Для каждого метода необходимо измерить и сравнить:\n",
    "\n",
    "#### Эффективность ресурсов:\n",
    "- **Количество обучаемых параметров** (в % от общего числа параметров модели)\n",
    "- **Потребление GPU памяти** (в GB во время обучения и инференса)\n",
    "- **Время обучения** (сек/эпоху)\n",
    "- **Скорость инференса** (токенов/секунду)\n",
    "\n",
    "#### Качество результатов:\n",
    "- **Основные метрики** в зависимости от задачи (ROUGE для суммаризации, BLEU для перевода)\n",
    "- **Стабильность обучения** (сходимость функции потерь)\n",
    "- **Качественный анализ** выходных текстов\n",
    "\n",
    "#### Требования к отчету:\n",
    "1. **Сравнительная таблица** всех протестированных методов\n",
    "2. **Графики Pareto-frontier**: эффективность vs качество\n",
    "3. **Обоснованные рекомендации** по выбору метода для различных сценариев\n",
    "4. **Анализ компромиссов** между точностью и эффективностью\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1: Parameter Efficient Fine-Tuning (PEFT) моделей для суммаризации текстов\n",
    "\n",
    "Цель задания — научиться применять современные методы параметрически-эффективного дообучения (PEFT) для настройки больших языковых моделей на задачу суммаризации текстов. Вы реализуете различные PEFT подходы: LoRA, QLoRA из библиотеки PEFT.\n",
    "\n",
    "#### Задачи:\n",
    "\n",
    "1. **Выбор датасета**:\n",
    "   - Загрузите датасет для задачи суммаризации, например, датасет `CNN/DailyMail`, содержащий новостные статьи и их краткие содержания (референсы). Используйте библиотеку `datasets` для загрузки данных.\n",
    "   - Обратите внимание на то, что вы можете использовать и другие подходящие датасеты для суммаризации (например, XSum).\n",
    "\n",
    "2. **Предобработка данных**:\n",
    "   - **Разделите** данные на обучающую и тестовую выборки.\n",
    "   - **Очистите текст** от лишних символов, специальных токенов и пробелов.\n",
    "   - **Подготовьте данные** в формате, подходящем для выбранной модели:\n",
    "     - Для GPT-2 вам нужно будет подать текст целиком (входной текст + референсное суммирование в одном формате).\n",
    "     - Для T5 модель требует форматировать входные данные в виде `summarize: <текст>` для текстов, которые нужно суммировать.\n",
    "\n",
    "3. **Создание модели**:\n",
    "   - **GPT-2**:\n",
    "     - Импортируйте предобученную модель `GPT2LMHeadModel` из библиотеки Hugging Face.\n",
    "     - Модель GPT-2 изначально не предобучена для задачи суммаризации, поэтому требуется её дообучение на подходящих данных.\n",
    "     - Поддержите правильное управление длиной сгенерированного текста при инференсе, чтобы обеспечить краткость суммаризаций.\n",
    "   \n",
    "   - **T5**:\n",
    "     - Для T5 используйте модель `T5ForConditionalGeneration`. T5 уже предобучена на множестве задач, включая суммаризацию, поэтому она лучше подходит для данной задачи.\n",
    "     - В отличие от GPT-2, модель T5 обучена на задаче, где входной текст — это задание (например, \"summarize:\") + текст для обработки, а выход — краткое содержание. Это нужно учесть при подготовке данных.\n",
    "     - Импортируйте модель из библиотеки Hugging Face и настройте её для использования на задаче суммаризации.\n",
    "\n",
    "4. **Настройка параметров обучения**:\n",
    "   - Настройте параметры обучения для обеих моделей:\n",
    "     - Количество эпох.\n",
    "     - Размер батча.\n",
    "     - Скорость обучения.\n",
    "     - Выберите оптимизатор (например, AdamW).\n",
    "   - Для T5 используйте кросс-энтропийную функцию потерь (`CrossEntropyLoss`), так как это задача генерации текста с \"условной вероятностью\". Для GPT-2 используйте ту же функцию с учётом автогрегрессивного генерационного процесса.\n",
    "\n",
    "5. **PEFT Fine-tuning модели** (основная часть задания):\n",
    "   - **Обязательно** реализуйте дообучение с использованием различных PEFT методов:\n",
    "     - **LoRA (Low-Rank Adaptation)**: Настройте параметры rank (r), alpha, dropout\n",
    "     - **QLoRA (Quantized LoRA)**: Используйте 4-bit квантизацию с LoRA\n",
    "     - **AdaLoRA**: Адаптивное изменение ранга во время обучения\n",
    "     - **Дополнительно**: попробуйте IA³ (Infused Adapter by Inhibiting and Amplifying Inner Activations) или Prompt Tuning\n",
    "   - Для каждого PEFT метода:\n",
    "     - Настройте специфические гиперпараметры (rank, alpha, target_modules)\n",
    "     - Измерьте количество обучаемых параметров\n",
    "     - Зафиксируйте время обучения и потребление памяти\n",
    "   - Используйте класс `Trainer` или `SFTTrainer` из библиотеки `trl` для процесса дообучения\n",
    "   - Сравните результаты full fine-tuning с PEFT подходами (опционально)\n",
    "\n",
    "6. **Инференс**:\n",
    "   - Используйте обе модели для генерации кратких содержаний на тестовой выборке.\n",
    "   - Подготовьте несколько примеров суммаризаций и выведите результаты для каждой модели.\n",
    "   - Для инференса используйте разные стратегии декодирования:\n",
    "     - **Greedy decoding** (жадный поиск).\n",
    "     - **Beam search** (поиск по нескольким лучам).\n",
    "     - **Sampling** (стохастическая генерация с использованием вероятностей).\n",
    "   - Сравните результаты, чтобы понять, как разные стратегии влияют на качество суммаризаций.\n",
    "\n",
    "7. **Сравнительная оценка PEFT методов**:\n",
    "   - Создайте сравнительную таблицу для всех протестированных методов:\n",
    "     - Количество обучаемых параметров (в % от общего числа параметров модели)\n",
    "     - Время обучения на эпоху\n",
    "     - Потребление GPU памяти\n",
    "     - Качество суммаризации по метрикам ROUGE и BLEU\n",
    "   - Оцените качество суммаризаций с использованием метрик:\n",
    "     - **ROUGE-1, ROUGE-2, ROUGE-L** — для оценки точности и полноты суммаризаций\n",
    "     - **BLEU** — для оценки схожести с референсным текстом\n",
    "   - Проанализируйте trade-off между эффективностью обучения и качеством результата\n",
    "\n",
    "#### Ожидаемые результаты:\n",
    "- **Код с реализацией различных PEFT методов** для выбранной модели (GPT-2 или T5)\n",
    "- **Сравнительный анализ** всех протестированных PEFT подходов с детальными метриками эффективности\n",
    "- **Отчет** с обоснованием выбора оптимального PEFT метода для задачи суммаризации\n",
    "- **Примеры суммаризаций** для каждого PEFT метода с качественным анализом различий\n",
    "- **Рекомендации** по выбору PEFT подхода в зависимости от ограничений по ресурсам\n",
    "\n",
    "#### Рекомендуемые ресурсы:\n",
    "- **[PEFT Documentation](https://huggingface.co/docs/peft/index)** - основная документация библиотеки PEFT\n",
    "- **[LoRA Developer Guide](https://huggingface.co/docs/peft/developer_guides/lora)** - детальное руководство по LoRA\n",
    "- **[QLoRA Implementation](https://huggingface.co/docs/peft/developer_guides/quantization)** - квантизация и QLoRA\n",
    "- [Документация Hugging Face Transformers](https://huggingface.co/docs/transformers/index)\n",
    "- [Документация Hugging Face Datasets](https://huggingface.co/docs/datasets/index)\n",
    "- **[PEFT Examples](https://github.com/huggingface/peft/tree/main/examples)** - примеры использования различных PEFT методов\n",
    "- **[TRL SFTTrainer](https://huggingface.co/docs/trl/sft_trainer)** - для supervised fine-tuning\n",
    "\n",
    "#### Критерии оценки:\n",
    "- **Корректность реализации PEFT методов** (30%) - правильная настройка и применение различных PEFT подходов\n",
    "- **Полнота сравнительного анализа** (25%) - детальное сравнение методов по всем указанным метрикам\n",
    "- **Качество сгенерированных суммаризаций** (20%) - оценка по ROUGE/BLEU метрикам\n",
    "- **Глубина анализа и выводов** (15%) - обоснованные рекомендации по выбору PEFT метода\n",
    "- **Четкость и структурированность отчета** (10%) - качество документации процесса и результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2: Применение различных PEFT методов для задачи машинного перевода\n",
    "\n",
    "**Цель задания**: Сравнить эффективность различных Parameter Efficient Fine-Tuning (PEFT) подходов для задачи автоматического перевода с английского на русский. В рамках задания необходимо реализовать и сравнить минимум 3 различных PEFT метода, проанализировать их влияние на качество перевода и эффективность обучения.\n",
    "\n",
    "#### Задачи:\n",
    "\n",
    "1. **Выбор датасета**:\n",
    "   - Загрузите параллельный датасет для перевода, например, [Opus Books](https://huggingface.co/datasets/Helsinki-NLP/opus_books/viewer/en-ru), который содержит тексты на английском языке и их переводы на русский.\n",
    "   - Используйте библиотеку `datasets` для загрузки и обработки данных. Убедитесь, что данные содержат параллельные тексты для обучения модели переводу.\n",
    "\n",
    "2. **Предобработка данных**:\n",
    "   - **Разделите данные** на обучающую и тестовую выборки (например, 80% для обучения и 20% для тестирования).\n",
    "   - **Очистите текст**, удалив лишние пробелы и специальные символы, которые могут повлиять на обучение модели.\n",
    "   - **Подготовьте данные** в формате, подходящем для выбранной модели:\n",
    "     - Для GPT-2 данные должны быть в виде последовательности токенов, где исходный текст и перевод разделены специальными символами (например, `<|startoftext|>` для начала текста и `<|endoftext|>` для его конца).\n",
    "     - Для T5 данные подаются в формате задачи перевода: входной текст начинается с задания `\"translate English to Russian: <текст на английском>\"`, а на выходе модель должна сгенерировать перевод.\n",
    "\n",
    "3. **Создание модели**:\n",
    "   - **GPT-2**:\n",
    "     - Импортируйте предобученную модель `GPT2LMHeadModel` из библиотеки Hugging Face.\n",
    "     - GPT-2 изначально не обучена для задачи перевода, поэтому нужно будет использовать специальную подготовку данных и дообучение на параллельных текстах.\n",
    "     - Убедитесь, что модель настроена для генерации текста, ограничивая длину вывода для перевода.\n",
    "   \n",
    "   - **T5**:\n",
    "     - Импортируйте модель `T5ForConditionalGeneration`, которая предобучена на множестве задач, включая перевод. T5 — это модель с условной генерацией, которая использует специальную задачу (`task`) для перевода.\n",
    "     - Подготовьте модель для выполнения задачи перевода с английского на русский, используя предобученные веса и формат входных данных.\n",
    "\n",
    "4. **Настройка параметров обучения**:\n",
    "   - Настройте параметры обучения для обеих моделей:\n",
    "     - Количество эпох (например, 3-5 эпох).\n",
    "     - Размер батча (например, 16 или 32).\n",
    "     - Скорость обучения (рекомендуется начать с 5e-5 и адаптировать в зависимости от потерь на валидации).\n",
    "   - Используйте подходящие оптимизаторы, такие как AdamW, и функцию потерь для задачи перевода:\n",
    "     - Для T5 подойдёт стандартная кросс-энтропийная функция потерь.\n",
    "     - Для GPT-2 используйте ту же функцию с учётом последовательной генерации текста (автогрегрессии).\n",
    "\n",
    "5. **Сравнительное исследование PEFT методов** (ключевая часть задания):\n",
    "   - **Обязательно** реализуйте и сравните следующие PEFT подходы:\n",
    "     - **LoRA**: Настройте различные значения rank (4, 8, 16, 32), alpha (16, 32, 64)\n",
    "     - **QLoRA**: Комбинация 4-bit квантизации с LoRA для экономии памяти\n",
    "     - **AdaLoRA**: Адаптивное изменение ранга с бюджетом параметров\n",
    "     - **Дополнительные методы** (на выбор): IA³, Prefix Tuning, P-Tuning v2, или (IA)³\n",
    "   - Для каждого PEFT метода проведите **grid search** по ключевым гиперпараметрам\n",
    "   - **Baseline**: обязательно сравните с полным fine-tuning (если позволяют ресурсы)\n",
    "   - Зафиксируйте для каждого эксперимента:\n",
    "     - Процент обучаемых параметров от общего числа\n",
    "     - Потребление GPU памяти (в GB)\n",
    "     - Время обучения на эпоху\n",
    "     - Скорость инференса (токенов/сек)\n",
    "   - Используйте `SFTTrainer` или `Trainer` для стабильного процесса обучения\n",
    "\n",
    "6. **Инференс**:\n",
    "   - Используйте дообученную модель для перевода текстов с английского на русский на тестовой выборке.\n",
    "   - Подготовьте несколько примеров для перевода и выведите результаты:\n",
    "     - Для GPT-2 используйте автогрегрессивное декодирование текста.\n",
    "     - Для T5 применяйте стандартные стратегии декодирования (например, greedy decoding или beam search).\n",
    "   \n",
    "7. **Комплексная оценка и анализ PEFT методов**:\n",
    "   - **Автоматические метрики**:\n",
    "     - **BLEU** (corpus-level и sentence-level) — основная метрика для машинного перевода\n",
    "     - **chrF** — character-level F-score для более точной оценки морфологически богатых языков\n",
    "     - **COMET** — нейронная метрика качества перевода (если позволяют ресурсы)\n",
    "     - **ROUGE** — для дополнительной оценки похожести\n",
    "   - **Создайте детальную сравнительную таблицу**:\n",
    "     - Эффективность (% параметров, время, память) vs Качество (BLEU, chrF)\n",
    "     - Pareto-frontier анализ: какие методы обеспечивают лучший trade-off\n",
    "   - **Качественный анализ**: \n",
    "     - Проанализируйте примеры переводов от каждого PEFT метода\n",
    "     - Определите типы ошибок, характерные для каждого подхода\n",
    "     - Оцените стабильность качества на различных типах текстов\n",
    "\n",
    "#### Ожидаемые результаты:\n",
    "- **Исследовательский код** с реализацией всех протестированных PEFT методов\n",
    "- **Научно-обоснованный отчет** с детальным сравнением эффективности PEFT подходов\n",
    "- **Визуализации**: графики Pareto-frontier, сравнительные диаграммы по метрикам\n",
    "- **Практические рекомендации**: когда использовать каждый PEFT метод в зависимости от ограничений\n",
    "- **Воспроизводимые результаты**: четкие инструкции по повторению экспериментов\n",
    "\n",
    "#### Рекомендуемые ресурсы:\n",
    "- **[PEFT Documentation](https://huggingface.co/docs/peft/index)** - полная документация библиотеки PEFT\n",
    "- **[LoRA Paper & Implementation](https://arxiv.org/abs/2106.09685)** - оригинальная статья LoRA\n",
    "- **[QLoRA Paper](https://arxiv.org/abs/2305.14314)** - квантизованный LoRA подход\n",
    "- **[AdaLoRA Paper](https://arxiv.org/abs/2303.10512)** - адаптивный LoRA\n",
    "- **[PEFT Task Guides](https://huggingface.co/docs/peft/task_guides/translation)** - гайды по применению PEFT для разных задач\n",
    "- [Документация Hugging Face Transformers](https://huggingface.co/docs/transformers/index)\n",
    "- [Документация Hugging Face Datasets](https://huggingface.co/docs/datasets/index)\n",
    "- **[BitsAndBytes](https://huggingface.co/docs/bitsandbytes/index)** - для квантизации в QLoRA\n",
    "- **[TRL Library](https://huggingface.co/docs/trl/index)** - для эффективного обучения\n",
    "\n",
    "#### Критерии оценки:\n",
    "- **Корректность реализации PEFT методов** (35%) - правильная настройка минимум 3 различных PEFT подходов\n",
    "- **Качество сравнительного анализа** (25%) - детальное сравнение по всем метрикам эффективности и качества\n",
    "- **Научная обоснованность выводов** (20%) - аргументированные рекомендации по выбору методов\n",
    "- **Воспроизводимость результатов** (10%) - четкие инструкции и фиксированные семена\n",
    "- **Качество перевода** (10%) - достижение конкурентоспособных результатов по BLEU/chrF метрикам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
