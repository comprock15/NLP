{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMVgo2YLJ4YA"
      },
      "source": [
        "# **Важно!**\n",
        "\n",
        "Домашнее задание состоит из нескольких задач, которые вам нужно решить.\n",
        "*   Баллы выставляются по принципу выполнено/невыполнено.\n",
        "*   За каждую выполненую задачу вы получаете баллы (количество баллов за задание указано в скобках).\n",
        "\n",
        "**Инструкция выполнения:** Выполните задания в этом же ноутбуке (места под решения **КАЖДОЙ** задачи обозначены как **#НАЧАЛО ВАШЕГО РЕШЕНИЯ** и **#КОНЕЦ ВАШЕГО РЕШЕНИЯ**)\n",
        "\n",
        "**Как отправить задание на проверку:** Вам необходимо сохранить ваше решение в данном блокноте и отправить итоговый **файл .IPYNB** на учебной платформе в **стандартную форму сдачи домашнего задания.**\n",
        "\n",
        "**Срок проверки преподавателем:** домашнее задание проверяется **в течение 3 дней после дедлайна сдачи** с предоставлением обратной связи\n",
        "\n",
        "# **Прежде чем проверять задания:**\n",
        "\n",
        "1. Перезапустите **ядро (restart the kernel)**: в меню, выбрать **Ядро (Kernel)**\n",
        "→ **Перезапустить (Restart)**\n",
        "2. Затем **Выполнить** **все ячейки (run all cells)**: в меню, выбрать **Ячейка (Cell)**\n",
        "→ **Запустить все (Run All)**.\n",
        "\n",
        "После ячеек с заданием следуют ячейки с проверкой **с помощью assert.**\n",
        "\n",
        "Если в коде есть ошибки, assert выведет уведомление об ошибке.\n",
        "\n",
        "Если в коде нет ошибок, assert отработает без вывода дополнительной информации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOJYktaoJ4YC"
      },
      "source": [
        "### **Задание**\n",
        "\n",
        "В этом задании вам предстоит разобраться с энкодерами трансформеров на примере задач классификации и работы с различными моделями семейства BERT и XLNet. Задание состоит из трёх частей, каждая из которых поможет вам глубже понять работу трансформеров, их обучение и использование для решения задач обработки естественного языка (NLP).\n",
        "\n",
        "#### Часть 1: Классификация с использованием модели BERT и стандартного цикла обучения (без `Trainer`)\n",
        "1. В этой части вам необходимо решить задачу текстовой классификации (например, классификация отзывов на положительные и отрицательные) с использованием предобученной модели из семейства BERT (например, `bert-base-uncased`).\n",
        "2. Используйте стандартный цикл обучения, без использования класса `Trainer`. Это значит, что вы будете самостоятельно определять цикл обучения, включая шаги прямого прохода, вычисления функции потерь, обратного прохода и обновления весов модели.\n",
        "3. Краткое руководство:\n",
        "   - Загрузите датасет, подходящий для задачи классификации (например, IMDb или другой).\n",
        "   - Используйте библиотеку Huggingface `transformers` для загрузки модели и токенизатора.\n",
        "   - Реализуйте процесс токенизации входных данных и подготовки батчей.\n",
        "   - Напишите цикл обучения с использованием библиотеки `torch`:\n",
        "     - Прямой проход (forward pass) через модель.\n",
        "     - Вычисление функции потерь.\n",
        "     - Обратный проход (backward pass) и обновление весов.\n",
        "   - Оцените точность (accuracy) модели на тестовой выборке.\n",
        "\n",
        "#### Часть 2: Классификация с кастомной \"головой\" на базе BERT и использованием класса `Trainer`\n",
        "1. В этой части вам необходимо решить задачу классификации с использованием модели BERT, но с добавлением кастомной \"головы\" (custom head) — нового слоя или нескольких слоев поверх базовой модели BERT.\n",
        "2. Описание задачи:\n",
        "   - В качестве основы возьмите ту же модель `bert-base-uncased`, но добавьте свой собственный классификатор (например, дополнительные линейные слои или слои с нелинейностью).\n",
        "   - Используйте класс `Trainer` из библиотеки `transformers` для упрощения процесса обучения.\n",
        "   - Настройте гиперпараметры (шаг обучения, количество эпох и т.д.) и запустите обучение модели.\n",
        "3. Краткое руководство:\n",
        "   - Определите кастомную голову, которая будет использовать выходы последнего слоя BERT для классификации.\n",
        "   - Создайте новый класс, наследующий от `torch.nn.Module`, и включите в него кастомные слои.\n",
        "   - Настройте процесс обучения с помощью класса `Trainer`:\n",
        "     - Определите функции потерь и метрики.\n",
        "     - Укажите параметры оптимизатора и планировщика (scheduler).\n",
        "   - Оцените производительность модели с кастомной \"головой\" на тестовой выборке и сравните с результатами первой части.\n",
        "\n",
        "#### Часть 3: Сравнение метрик BERT и XLNet на задаче токен-классификации\n",
        "1. В этой части вам необходимо сравнить производительность двух моделей — BERT и XLNet — на задаче токен-классификации (например, задача распознавания именованных сущностей — Named Entity Recognition, NER).\n",
        "2. Задача:\n",
        "   - Возьмите готовый датасет для токен-классификации (например, CoNLL-2003 NER Dataset).\n",
        "   - Используйте модели BERT (`bert-base-cased`) и XLNet (`xlnet-base-cased`) для решения этой задачи.\n",
        "   - Обучите обе модели с использованием класса `Trainer` и сравните их производительность по метрикам точности (accuracy), полноты (recall), F1-меры.\n",
        "3. Краткое руководство:\n",
        "   - Загрузите и предобработайте данные для задачи токен-классификации (например, используя библиотеку `datasets`).\n",
        "   - Для каждой из моделей настройте обучение с использованием класса `Trainer`.\n",
        "   - Обучите модели и получите метрики на тестовой выборке.\n",
        "   - Сравните результаты: какие различия наблюдаются между производительностью BERT и XLNet на задаче токен-классификации? Как это можно объяснить с точки зрения архитектуры моделей?\n",
        "\n",
        "#### Ожидаемый результат:\n",
        "- Все задачи решаются в одном Jupyter notebook с подробным описанием процесса решения задачи, кодом и анализом результатов.\n",
        "- В третьей части задания необходимо сделать выводы по сравнению производительности моделей BERT и XLNet.\n",
        "- Включите графики метрик и таблицы результатов для наглядности.\n",
        "\n",
        "#### Ресурсы:\n",
        "- Библиотеки: `transformers`, `torch`, `datasets`, `numpy`, `pandas`.\n",
        "- Датасеты: IMDb (или любой другой для текстовой классификации), CoNLL-2003 для токен-классификации.\n",
        "\n",
        "\n",
        "Рекомендуемые ресурсы:\n",
        "- Документация Hugging Face Transformers https://huggingface.co/docs/transformers/index\n",
        "- Документация Hugging Face Datasets https://huggingface.co/docs/datasets/index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VZarI2QJ4YD"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Часть 1: Классификация с использованием модели BERT и стандартного цикла обучения (без Trainer)"
      ],
      "metadata": {
        "id": "6cOuG9XQK7oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим датасет IMDb с рецензиями на фильмы"
      ],
      "metadata": {
        "id": "9Q5PhGqHM9rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import tarfile\n",
        "\n",
        "# URL для загрузки\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "# Определите путь, по которому сохранить файл\n",
        "file_path = \"dataset.tar.gz\"\n",
        "dataset_name = \"aclImdb\"\n",
        "\n",
        "# Проверяем, существует ли распакованная директория\n",
        "if os.path.exists(file_path[:-7]):\n",
        "    print(f\"Директория {file_path[:-7]} уже существует. Загрузка и распаковка пропущены.\")\n",
        "else:\n",
        "    # Отправьте GET-запрос для загрузки файла\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
        "    block_size = 1024\n",
        "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
        "    with open(file_path, 'wb') as file:\n",
        "        for data in response.iter_content(block_size):\n",
        "            progress_bar.update(len(data))\n",
        "            file.write(data)\n",
        "    progress_bar.close()\n",
        "\n",
        "    # Проверяем, успешно ли загрузился файл\n",
        "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
        "        print(f\"Не удалось загрузить файл. Код состояния: {response.status_code}\")\n",
        "    else:\n",
        "        print(f\"Файл {file_path} успешно загружен.\")\n",
        "\n",
        "        # Директория, в которую вы хотите распаковать архив\n",
        "        extract_to_directory = \".\"\n",
        "\n",
        "        # Создайте объект tarfile\n",
        "        with tarfile.open(file_path, 'r:gz') as tar:\n",
        "            # Распакуйте все содержимое архива в указанную директорию\n",
        "            tar.extractall(extract_to_directory)\n",
        "\n",
        "        print(f\"Архив {file_path} успешно распакован в директорию {extract_to_directory}\")\n",
        "\n",
        "        # Удаляем TAR-файл после распаковки\n",
        "        os.remove(file_path)\n",
        "        print(f\"TAR-файл {file_path} был удален.\")\n",
        "\n",
        "        os.rename(dataset_name, \"dataset\")\n",
        "        print(f\"Директория {dataset_name} переименована в dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eobeOxAQbqDu",
        "outputId": "627c7efb-ab92-41cb-8fe8-0fd62f51399c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Директория dataset уже существует. Загрузка и распаковка пропущены.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохраняем данные из текстовых файлов в списки"
      ],
      "metadata": {
        "id": "sj5tBcxRgldl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Путь к датасету\n",
        "dataset_path = \"./dataset\"\n",
        "\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "\n",
        "for label_type in ['pos', 'neg']:\n",
        "    dir_name = os.path.join(dataset_path, 'train', label_type)\n",
        "    label = 1 if label_type == 'pos' else 0\n",
        "\n",
        "    # Чтение файлов train\n",
        "    for filename in os.listdir(dir_name):\n",
        "            file_path = os.path.join(dir_name, filename)\n",
        "\n",
        "            # Прочитать содержимое файла\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            # Добавляем текст и его метку к соответствующим спискам\n",
        "            train_texts.append(text)\n",
        "            train_labels.append(label)\n",
        "\n",
        "    # Чтение файлов test\n",
        "    for filename in os.listdir(dir_name):\n",
        "            file_path = os.path.join(dir_name, filename)\n",
        "\n",
        "            # Прочитать содержимое файла\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "\n",
        "            # Добавляем текст и его метку к соответствующим спискам\n",
        "            test_texts.append(text)\n",
        "            test_labels.append(label)\n",
        "\n",
        "print(f\" Text: {train_texts[0]}\\nLabel: {train_labels[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUaYtNpmgpkV",
        "outputId": "934d0ce6-9303-4e4d-f3d5-3e163c6266be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Text: An Insomniac's Nightmare is the story of a man's plunge into insanity. Having chronic Insomnia, Jack is plagued by hallucinations; causing him to try and determine what is real and what isn't.<br /><br />We find out interesting things about Jack near the end, and think that by the time the movie is over we will have a \"happily ever after\" Hollywood ending. Wrong. This is New York City, the place where nobody sleeps.<br /><br />Tess Nanavati (Writer and Director) has herself a good film in 'An Insomniac's Nightmare'. A talented filmmaker and writer (she made this film right after her High School Graduation), she has real potential and will be one to watch in the upcoming future.<br /><br />As I watched this short film I was constantly uncomfortable; between the music, bleak scenery, and realistic portrayal of an insomniac by Dominic Monaghan (as Jack), I desperately wanted to turn this off at times just to escape from it.\n",
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы будем использовать только часть датасета, так как на полном модель обучается очень долго :("
      ],
      "metadata": {
        "id": "1XXUL1y5nVVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "train_texts, train_labels = shuffle(train_texts, train_labels)\n",
        "test_texts, test_labels = shuffle(test_texts, test_labels)\n",
        "\n",
        "train_samples, test_samples = 2000, 500\n",
        "\n",
        "train_texts = train_texts[:train_samples]\n",
        "train_labels = train_labels[:train_samples]\n",
        "test_texts = test_texts[:test_samples]\n",
        "test_labels = test_labels[:test_samples]"
      ],
      "metadata": {
        "id": "eC6PllWXnS1P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем модель и токенизатор"
      ],
      "metadata": {
        "id": "sJ-_5T3qjGKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "# Загружаем токенизатор и модель\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Перемещаем модель на GPU, если возможно\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Модель загружена на {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3kFmLDRjMMY",
        "outputId": "d3cb2c2e-d24c-49b8-ccef-74d982f8c05f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель загружена на cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация входных данных и подготовка батчей"
      ],
      "metadata": {
        "id": "Dm5RVVtgoEJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "def prepare_dataloader(texts, labels, batch_size=16):\n",
        "    # Токенизация\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512, # максимальное кол-во токенов в BERT\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    dataset = TensorDataset(\n",
        "        encodings['input_ids'],\n",
        "        encodings['attention_mask'],\n",
        "        torch.tensor(labels)\n",
        "    )\n",
        "\n",
        "    # Создание DataLoader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "train_loader = prepare_dataloader(train_texts, train_labels)\n",
        "test_loader  = prepare_dataloader(test_texts,  test_labels)"
      ],
      "metadata": {
        "id": "KOrZbE5anXQ7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Цикл обучения с использованием библиотеки torch:\n",
        "\n",
        "* Прямой проход (forward pass) через модель.\n",
        "* Вычисление функции потерь.\n",
        "* Обратный проход (backward pass) и обновление весов."
      ],
      "metadata": {
        "id": "gOYSrXSO0LMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Оптимизатор\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "# Функция потерь\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()  # режим обучения\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        # Перемещаем данные на устройство\n",
        "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "\n",
        "        # Обнуляем градиенты\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Прямой проход\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Вычисление потерь\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Обратный проход\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Полный цикл обучения\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Эпоха {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Обучение\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    print(f\"Потери обучения: {train_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRVawTNm0Kh0",
        "outputId": "720998a4-02a0-4a77-ee8c-733d60cbc925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/125 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оцените точность (accuracy) модели на тестовой выборке."
      ],
      "metadata": {
        "id": "8OSrrdEQ0WXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KwzK7xdJ4YE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a40208-fc97-47bb-ddd0-de17bbfb782d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: 100%|██████████| 32/32 [00:13<00:00,  2.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность на тестовой выборке: 0.9040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_accuracy(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Оценка точности модели\n",
        "    \"\"\"\n",
        "    model.eval()  # режим оценки\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # отключаем вычисление градиентов\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluation\"):\n",
        "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "\n",
        "            # Прямой проход\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Получаем предсказания\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Считаем правильные предсказания\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Оценка точности\n",
        "accuracy = evaluate_accuracy(model, test_loader, device)\n",
        "print(f\"Точность на тестовой выборке: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Часть 2: Классификация с кастомной \"головой\" на базе BERT и использованием класса Trainer"
      ],
      "metadata": {
        "id": "WvQ061qFLKhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определите кастомную голову, которая будет использовать выходы последнего слоя BERT для классификации."
      ],
      "metadata": {
        "id": "TfIRZD-IyomQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создайте новый класс, наследующий от torch.nn.Module, и включите в него кастомные слои."
      ],
      "metadata": {
        "id": "yuvNmGsiyslO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Настройте процесс обучения с помощью класса Trainer:\n",
        "*    Определите функции потерь и метрики.\n",
        "*    Укажите параметры оптимизатора и планировщика (scheduler)."
      ],
      "metadata": {
        "id": "mbODbonwyvKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оцените производительность модели с кастомной \"головой\" на тестовой выборке и сравните с результатами первой части."
      ],
      "metadata": {
        "id": "XXBbgFH_y0jw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C1bZKrrJ4YF"
      },
      "outputs": [],
      "source": [
        "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "raise NotImplementedError() # удалить эту строку в процессе решения\n",
        "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Часть 3: Сравнение метрик BERT и XLNet на задаче токен-классификации"
      ],
      "metadata": {
        "id": "Mx4aDfxOLPdm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7MUfAaiJ4YH"
      },
      "outputs": [],
      "source": [
        "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "raise NotImplementedError() # удалить эту строку в процессе решения\n",
        "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}