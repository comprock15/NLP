{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Важно!** \n",
    "\n",
    "Домашнее задание состоит из нескольких задач, которые вам нужно решить.\n",
    "*   Баллы выставляются по принципу выполнено/невыполнено.\n",
    "*   За каждую выполненую задачу вы получаете баллы (количество баллов за задание указано в скобках).\n",
    "\n",
    "**Инструкция выполнения:** Выполните задания в этом же ноутбуке (места под решения **КАЖДОЙ** задачи обозначены как **#НАЧАЛО ВАШЕГО РЕШЕНИЯ** и **#КОНЕЦ ВАШЕГО РЕШЕНИЯ**)\n",
    "\n",
    "**Как отправить задание на проверку:** Вам необходимо сохранить ваше решение в данном блокноте и отправить итоговый **файл .IPYNB** в личном сообщении Telegram.\n",
    "\n",
    "# **Прежде чем проверять задания:**\n",
    "\n",
    "1. Перезапустите **ядро (restart the kernel)**: в меню, выбрать **Ядро (Kernel)**\n",
    "→ **Перезапустить (Restart)**\n",
    "2. Затем **Выполнить** **все ячейки (run all cells)**: в меню, выбрать **Ячейка (Cell)**\n",
    "→ **Запустить все (Run All)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание — LangChain и инференс\n",
    "\n",
    "Цель: перевести домашнее задание на использование LangChain. В задании — 4 задачи, в том числе про LCEL и Structured Output. Для каждой задачи дан стартовый код."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Задачи (кратко)\n",
    "\n",
    "1. Task 1 — Быстрый старт LangChain: Prompt + LLM\n",
    "2. Task 2 — Chains и составные сценарии\n",
    "3. Task 3 — LCEL: сценарий с несколькими шагами логики\n",
    "4. Task 4 — Structured Output: схемы и валидация\n",
    "\n",
    "Дальше следует подробное описание каждой задачи с критериями и стартовым кодом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей (выполните в среде развертывания один раз)\n",
    "# !pip install langchain openai pydantic\n",
    "\n",
    "OPENROUTER_API_KEY =  \"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\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1 — Quick LangChain prompt\n",
    "\n",
    "Цель: собрать минимальную LCEL-цепочку «prompt → LLM → парсер», которая возвращает краткий ответ модели на запрос \"Механизм внимания\".\n",
    "\n",
    "Acceptance criteria:\n",
    "- Используется `ChatPromptTemplate` и `ChatOpenAI` (или совместимая чат-модель), соединённые через LCEL (`|`).\n",
    "- В цепочку добавлен `StrOutputParser` и приведён пример вызова `chain.invoke(...)`.\n",
    "- В примере показано, где брать API-ключ (например, из переменной окружения `OPENROUTER_API_KEY`).\n",
    "\n",
    "Starter code:\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Ты отвечаешь кратко и по делу на русском языке.\"),\n",
    "    (\"human\", \"Объясни тему: {topic}\"),\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-5-nano\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\", OPENROUTER_API_KEY),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"topic\": \"LSTM\"})\n",
    "print(response)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 — Quick LangChain prompt\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 — Chains: составной pipeline\n",
    "\n",
    "Цель: собрать LCEL-пайплайн из двух шагов: (1) LLM подбирает факты по теме `\"transformers\"`, (2) отдельный шаг формирует резюме из заметок.\n",
    "\n",
    "Acceptance criteria:\n",
    "- Используется LangChain Expression Language (композиции `|`, `RunnablePassthrough`, `RunnableLambda`) вместо `SequentialChain`. `RunnablePassthrough` просто прокидывает вход дальше по цепочке без изменений. В LCEL его используют, когда нужно передать исходные данные как часть словаря в последующие шаги: например, `{\"topic\": RunnablePassthrough(), \"research_notes\": research_chain}` — так и оригинальная тема, и результаты ресёрча доступны в следующем промпте.\n",
    "- Шаблоны prompt'ов для ресёрча и итогового резюме разделены и связаны логикой.\n",
    "- Есть пример вызова `.invoke` c входным топиком.\n",
    "\n",
    "Starter code:\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-5-nano\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\", OPENROUTER_API_KEY),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "research_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Собери три факта по теме.\"),\n",
    "    (\"human\", \"Тема: {topic}\"),\n",
    "])\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Сделай короткое резюме на русском.\"),\n",
    "    (\"human\", \"Суммаризируй заметки: {research_notes}\"),\n",
    "])\n",
    "\n",
    "research_chain = research_prompt | llm | StrOutputParser()\n",
    "\n",
    "pipeline = (\n",
    "    {\n",
    "        \"topic\": RunnablePassthrough(),\n",
    "        \"research_notes\": research_chain,\n",
    "    }\n",
    "    | summary_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = pipeline.invoke(\"rnns\")\n",
    "print(response)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 — Исследование + резюме в одном chain\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 — LCEL: сложная логика в несколько шагов\n",
    "\n",
    "Цель: построить условный роутер запросов на LCEL с несколькими ветками и fallback.\n",
    "1 Дать определение диффузии и 2 сравнить RNN и Transformer\n",
    "\n",
    "Acceptance criteria:\n",
    "- Предусмотрена классификация интента (`RunnableLambda` добавляет ключ `intent`).\n",
    "- `RunnableBranch` маршрутизирует запрос в разные цепочки и содержит запасной путь.\n",
    "- Показан пример вызова `.invoke`, демонстрирующий выбранную ветку.\n",
    "\n",
    "Starter code / схема:\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-5-nano\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\", OPENROUTER_API_KEY),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "definition_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Дай определение: {question}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "comparison_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Сравни подходы: {question}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def classify(inputs):\n",
    "    question = inputs[\"question\"].lower()\n",
    "    if \"сравн\" in question:\n",
    "        intent = \"comparison\"\n",
    "    elif \"что такое\" in question or \"определ\" in question:\n",
    "        intent = \"definition\"\n",
    "    else:\n",
    "        intent = \"fallback\"\n",
    "    return {\"intent\": intent, **inputs}\n",
    "\n",
    "router = RunnableBranch(\n",
    "    (lambda data: data[\"intent\"] == \"comparison\", comparison_chain),\n",
    "    (lambda data: data[\"intent\"] == \"definition\", definition_chain),\n",
    "    RunnableLambda(lambda data: \"Не знаю, уточните запрос.\"),\n",
    ")\n",
    "\n",
    "workflow = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | RunnableLambda(classify)\n",
    "    | router\n",
    ")\n",
    "response1 = workflow.invoke(\"Что такое LSTM?\")\n",
    "\n",
    "\n",
    "print(response1)\n",
    "\n",
    "response2 = workflow.invoke(\"Сравни TF-IDF и BoW\")\n",
    "\n",
    "print(response2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 — LCEL: сложная логика в несколько шагов\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 — Structured Output\n",
    "\n",
    "Цель: использовать актуальный `PydanticOutputParser` для строгой схемы ответа для темы `LLM`.\n",
    "\n",
    "Acceptance criteria:\n",
    "- Описана Pydantic-модель с типами и описаниями полей.\n",
    "- Prompt включает `parser.get_format_instructions()` и цепочка вызывает `.invoke`.\n",
    "- Полученный результат приводится к `dict`/`BaseModel` и выводится.\n",
    "\n",
    "Starter code:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Fact(BaseModel):\n",
    "    title: str = Field(..., description=\"Короткий заголовок\")\n",
    "    summary: str\n",
    "    confidence: float\n",
    "\n",
    "class FactCollection(BaseModel):\n",
    "    topic: str\n",
    "    facts: list[Fact]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=FactCollection)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Формируй JSON со списком фактов.\"),\n",
    "    (\"human\", \"Тема: {topic}\\nФормат: {format_instructions}\"),\n",
    "]).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-5-nano\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\", OPENROUTER_API_KEY),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "structured_chain = prompt | llm | parser\n",
    "\n",
    "result = structured_chain.invoke({\"topic\": \"Attention\"})\n",
    "print(result)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 — Structured Output\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
